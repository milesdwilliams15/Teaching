---
title: "Hypothesis Testing"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T,
                      message = F,
                      warning = F,
                      dpi = 500,
                      out.width = "75%")
```



## What is a hypothesis?

A hypothesis in science is usually specified in terms of some equality between two things. It could be that the mean of *x* in group A is equal to the mean of *x* in group B. Or something like: the correlation between *x* and *y* is equal to zero.

Whatever it is, it takes the form of *something* being equal to *something else*.

In practice, hypotheses can get pretty sophistocated, but even more baroque hypotheses are nothing but a riff on this core formulation.

Let's use your presidential forecasts as an example. Say we really believe in the power of our forecasts. So, we hypothesize that our projected prediction for Biden's Electoral College vote is the same as his actual performance in 2020. Call this the *my model is perfect hypothesis* or MMP hypothesis for short.

We can specify this more formally. Let *V* be Trump's number of votes in 2020 and let *P* be the predicted number of votes. *D* is the difference between the two. The MMP hypothesis holds that:

MMP: *D = V - P = 0*

In orders, the difference between my prediction and the actual outcome will be zero.

Let me train a really simple model to put this hypothesis to the test. First I need to read in the data from my GitHub.

```{r}
library(tidyverse)
url <- "https://raw.githubusercontent.com/milesdwilliams15/Teaching/main/DPR%20201/Data/predict_prez.csv"
Data <- read_csv(url)
```

Next I need to train a linear regression model. I'll try out a quadratic equation where an incumbent's total votes in the Electoral College is a curvilinear function of his June net approval ratings.

```{r}
fit <- lm(incelectvotes ~ poly(juneapp, 2), data = Data)
```

Trump was the incumbent in 2020. According to Gallup, his net approval in June of that year was -19. I'll plug that value into my fitted linear model to get a prediction for the 2020 election in November.

```{r}
newdata <- tibble(juneapp = -19)
pred <- predict(fit, newdata)
pred # check it out
```

To win the presidential election you need 270 votes. The actual number Trump received in 2020 was 232. My model predicted he would only get 132. The direction of the prediction is the same as the outcome, but Trump clearly overperformed relative to my model's forecast. In short, contrary to my hypothesis that *D = V - P = 0*, instead *D = V - P > 0*.

Clearly the data don't support the MMP hypothesis. So we should reject it, right?

Well, it's actually more complicated than that. Remeber our discussion about uncertainty when we make inferences? Before we can think about rejecting or not rejecting any hypotheses, we need to take uncertainty into account. We need to run a **hypothesis test**.


## What is a hypothesis test?

A hypothesis test is a statistical test that reports the likelihood of observing the relationship, difference, or value you estimate with your data assuming your hypothesis is true. The idea is that differences between reality and our idealized hypotheses may simply be the product of random chance. Just like with using samples to make inferences to populations, we can use statistics to make inferences about hypotheses.

The likelihood, or probability, of calculating an estimate with our data, assuming our hypothesis is true, is called a *p-value*. The *p* stands for *probability*. A p-value can be anywhere from 0 to 1. A value cose to 0 means that the likelihood of getting the estimate we got if our hypothesis is true is quite small. 

Think of a p-value as a way to quantify how embarassed our hypothesis is by the data. Generally, in political science research we use the *p < 0.05* threshold to decide whether to reject a hypothesis. If the p-value is less than 0.05, we reject the hypothesis. If it isn't, we say that we "fail to reject" the hypothesis.

Note that we never accept hypotheses. We only reject or fail to reject them. Think of it this way. Science is a process of eliminating or ruling out possibilities to home in on the truth. This is a never-ending process. To say we accept a hypothesis would be to say we don't think other explanations or forces could be at work that would explain it away. That's just bad science.

Confidence intervals, which we talked about last time, have a direct correspondence with with p-values. Think of them as a collection of p-values. 95\% confidence intervals in particular directly correspond with the 0.05 threshold for p-values. If some value is contained within the 95\% confidence intervals, then our estimate is not statistically distinguishable from that value at the *p < 0.05* threshold.

If you put this all together, it means that if we want to test the MMP hypothesis, we need to get a confidence interval for our prediction. If zero falls within the bounds of the interval, then even though my model's prediction doesn't exactly match Trump's performance I cannot reject the hypothesis that my model is perfect.

We can tell R we want the confidence intervals for a prediction by udating some commands in the `predict()` function. All we need to do is write:

```{r}
pred_ci <- predict(fit, newdata, 
                   se.fit = T,
                   interval = "confidence")
pred_ci # print
```

We can see from the output that the function is doing a lot more under the hood now. The main output to pay attention to is `$fit`. We can look at that specifically by writing:

```{r}
pred_ci$fit
```

The output tells us the model's prediction and it gives us the upper and lower bounds of the 95\% confidence intervals.

Should we reject the MMP hypothesis? Based on these results, we should. Trump's actual number of votes in 2020 (238) doesn't fall within the bounds of the confidence intervals.


## More on p-values

If we wanted to construct a p-value for our prediction based on the MMP hypothesis, we would compute a test statistic. This is generally the value of our estimate relative to its standard error. Depending on the estimate in question, this could be a t-value, or z-value, or chi-squared, etc. 

Each one has its own theoreticl distribution under a particular hypothesis. In the case of my model's prediction the MMP hypothesis, this distribution looks something like this:

```{r echo=FALSE}
library(geomtextpath)
library(coolorrr)
set_palette()
set_theme()
est_range <- seq(-5, 5, by = 0.01)
prob <- dt(x = est_range,
                df = pred_ci$df)
est <- (pred_ci$fit[1] - 238)/pred_ci$se.fit
est_prob <- 2 * pt(-abs(est), df = pred_ci$df)
ggplot() +
  aes(
    x = est_range,
    y = prob,
    fill = est_range > est
  ) +
  geom_area(
    show.legend = F
  ) +
  geom_textvline(
    xintercept = est,
    label = "My Prediction Statistic",
    color = dual[2]
  ) +
  geom_textvline(
    xintercept = 0,
    label = "If the MMP hypothesis were true",
    color = dual[1]
  ) +
  ggpal(
    aes = "fill",
    type = "binary"
  ) +
  labs(
    x = "Prediction Statistics under the MMP Hypothesis\n(prediction / std. error)",
    y = "Density",
    title = "How surprising is my model's prediction?"
  )
```

The statistic for my prediction is the prediction itself divided by its standard error. The standard error for a forecast is calculated differently from the standard error of a mean, but the idea is the same.

The p-value for my prediction given the MMP hypothesis is quite small, which is clear from the above figure. Specifically, it's `r round(est_prob, 3)`. That means that my hypothesis should be really, really embarrassed by the data.

Any time we calculate a p-value it comes from a distribution like the one above. Not all distributions are the same, but R knows how to give you the right p-value based on the specific hypothesis test you want to run.


## Try it out

What happens if you use a simple linear model rather than the quadratic one I estimated? Do you get a better prediction? Do you still reject the MMP hypothesis?
